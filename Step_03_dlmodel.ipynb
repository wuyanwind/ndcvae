{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfda06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat, savemat\n",
    "import logging\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd825d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deel_leaning based Spatial_Temporal classification model\n",
    "class MLPSpatialFilter(nn.Module):\n",
    "    def __init__(self, num_ROI, num_hidden, activation):\n",
    "        super(MLPSpatialFilter, self).__init__()\n",
    "        self.fc11 = nn.Linear(num_ROI, num_ROI)\n",
    "        self.fc12 = nn.Linear(num_ROI, num_ROI)\n",
    "        self.fc21 = nn.Linear(num_ROI, num_hidden)\n",
    "        self.fc22 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.fc23 = nn.Linear(num_ROI, num_hidden)\n",
    "        self.value = nn.Linear(num_hidden, num_hidden)\n",
    "        self.activation = nn.__dict__[activation]()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = dict()\n",
    "        x = self.activation(self.fc12(self.activation(self.fc11(x))) + x)\n",
    "        x = self.activation(self.fc22(self.activation(self.fc21(x))) + self.fc23(x))\n",
    "        out['value'] = self.value(x)\n",
    "        out['value_activation'] = self.activation(out['value'])\n",
    "        return out['value_activation']\n",
    "\n",
    "class PatchEmbedding_Linear(nn.Module):\n",
    "    # what are the proper parameters set here?\n",
    "    def __init__(self, in_channels=100, patch_size=4, emb_size=100, seq_length=225):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        # change the conv2d parameters here\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b (w s) c-> b w (s c)', s=patch_size),\n",
    "            nn.Linear(patch_size * in_channels, emb_size)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((seq_length // patch_size) + 1, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # position\n",
    "        x += self.positions\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size=100,\n",
    "                 num_heads=5,\n",
    "                 drop_p=0.,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=3, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "class RegressionHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=100, n_para=10):\n",
    "        super().__init__()\n",
    "        self.rghead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_para)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.rghead(x)\n",
    "        return out\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=100, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.clshead(x)\n",
    "        return out\n",
    "        \n",
    "class Regressor(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 num_ROI = 100,\n",
    "                 num_hidden = 100,\n",
    "                 activation ='ELU',\n",
    "                 in_channels=100,\n",
    "                 patch_size=4,\n",
    "                 emb_size=100,\n",
    "                 seq_length=225,\n",
    "                 depth=5,\n",
    "                 n_para=10,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            MLPSpatialFilter(num_ROI, num_hidden, activation),\n",
    "            PatchEmbedding_Linear(in_channels, patch_size, emb_size, seq_length),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
    "            ClassificationHead(emb_size, n_para)\n",
    "        )\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "class sz_cn_Dataset(Dataset):\n",
    "    def __init__(self, x=None,y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x,y\n",
    "\n",
    "def trainer(train_loader, model, criterion, optimizer, args_params):\n",
    "    # args_params: potential parameter inputs, could be \"device\",\"logger\"\n",
    "    device = args_params['device']\n",
    "    logger = args_params['logger']\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    start_time = time.time()\n",
    "    for batch_idx, sample_batch in enumerate(train_loader):\n",
    "        # load data\n",
    "        data = sample_batch[0].to(torch.float32)\n",
    "        label = sample_batch[1].to(torch.float32)\n",
    "        # training process\n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(data)\n",
    "        loss = criterion(model_output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.data.view(1))\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print_s = \"batch_idx_{}_time_{}_train_loss_{}\".format(batch_idx, time.time() - start_time, train_loss[-1])\n",
    "            logger.info(print_s)\n",
    "    train_loss = torch.cat(train_loss).cpu().numpy()\n",
    "    return train_loss\n",
    "# END TRAIN\n",
    "\n",
    "\n",
    "# START VALIDATE FUNC\n",
    "def validater(val_loader, model, criterion, args_params):\n",
    "    # switch to evaluate mode\n",
    "    device = args_params['device']\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample_batch in enumerate(val_loader):\n",
    "            data = sample_batch[0].to(torch.float32)\n",
    "            label = sample_batch[1].to(torch.float32)\n",
    "            model_output = model(data)\n",
    "            loss = criterion(model_output, label)\n",
    "            accuracy_temp = np.mean(((model_output > 0.5) == label).cpu().numpy())\n",
    "            accuracy.append(accuracy_temp)\n",
    "            val_loss.append(loss.data.view(1))\n",
    "    val_loss = torch.cat(val_loss).cpu().numpy()\n",
    "    accuracy_epoch = np.mean(np.array(accuracy))\n",
    "    print(accuracy_epoch)\n",
    "    return val_loss, accuracy_epoch\n",
    "# END VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters and optimization parameters\n",
    "net = Regressor(num_ROI = 100, num_hidden = 100, activation ='ELU', in_channels=100,\n",
    "             patch_size=1, emb_size=100, seq_length=225, depth=2, n_para=1).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "batch_size = 16\n",
    "train = 'train_data'\n",
    "test = 'test_data'\n",
    "arch = 'Regressor'\n",
    "device = 'cpu'\n",
    "lr=0.001\n",
    "epoch=30\n",
    "resume=0\n",
    "workers=0\n",
    "model_id = 1\n",
    "ds_all = loadmat(r'ds_dl.mat')\n",
    "ts_nc_sz = ds_all['ts_nc_sz']\n",
    "ts_label = ds_all['ts_label']\n",
    "all_dataset = sz_cn_Dataset(x=ts_nc_sz,y=ts_label)\n",
    "train_size = int(len(all_dataset) * 0.7)\n",
    "test_size = len(all_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(all_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16,\n",
    "                        shuffle=True, num_workers=0)\n",
    "result_root = './model_result/{}_the_model'.format(model_id)\n",
    "if not os.path.exists(result_root):\n",
    "    os.makedirs(result_root)\n",
    "# Define logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler(os.path.join('./outputs_{}.log'.format(arch)))\n",
    "handler.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "logger.info(\"============================= {} ====================================\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc4e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "start_epoch = 0\n",
    "best_result = np.Inf\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "accuracy = []\n",
    "start_time = time.time()\n",
    "save = 1\n",
    "for i in tqdm(range(1, epoch)):\n",
    "    train_lss_all = trainer(train_loader, net, criterion, optimizer, {'device': device, 'logger': logger})\n",
    "    # evaluate on validation set\n",
    "    test_lss_all, accuracy_epoch = validater(test_loader, net, criterion, {'device': device})\n",
    "    lr_scheduler.step()\n",
    "    print(epoch, lr_scheduler.get_lr()[0])\n",
    "    train_loss.extend([np.sum(np.array(train_lss_all)) / batch_size])\n",
    "    test_loss.extend([np.sum(np.array(test_lss_all)) / batch_size])\n",
    "    accuracy.append(accuracy_epoch)\n",
    "    print_s = 'Epoch {}: Time:{:6.2f}, '.format(epoch, time.time() - start_time) + \\\n",
    "              'Train Loss:{:06.5f}'.format(train_loss[-1]) + ', Test Loss:{:06.5f}'.format(test_loss[-1])\n",
    "    logger.info(print_s)\n",
    "    print(print_s)\n",
    "    is_best = test_loss[-1] < best_result\n",
    "    best_result = min(test_loss[-1], best_result)\n",
    "    if is_best:\n",
    "        torch.save({\n",
    "            'epoch': epoch, 'arch': arch, 'state_dict': net.state_dict(), 'best_result': best_result, 'lr': lr,\n",
    "            'train': train, 'test': test, 'optimizer': optimizer.state_dict()},\n",
    "            result_root + '/model_best.pth.tar')\n",
    "    if save:\n",
    "        # save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch, 'arch': arch, 'state_dict': net.state_dict(), 'best_result': best_result, 'lr': lr,\n",
    "            'train': train, 'test': test, 'optimizer': optimizer.state_dict()},\n",
    "            result_root + '/epoch_{}'.format(epoch))\n",
    "        savemat(result_root + '/train_test_error.mat', {'train_loss': train_loss, 'test_loss': test_loss, 'accuracy': accuracy})\n",
    "        savemat(result_root + '/train_test_loss_epoch{}.mat'.format(epoch), {'train_loss': train_lss_all, 'test_loss': test_lss_all})\n",
    "    # END MAIN_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0878e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
